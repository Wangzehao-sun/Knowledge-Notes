## 深度学习
[TOC]
#### 一、机器学习基础之梯度计算

> [!IMPORTANT]
>
> 随机梯度下降算法是机器学习算法的基础，所谓‘随机’即是指随机的从训练集中选取小批量样本来，并使用该小批量样本的梯度来近似整个训练集的梯度，从而根据**损失函数**对模型参数进行更新。
> $$
> ∇_xf(x) = [
> \frac{∂f(x)}{∂x_1},\frac{∂f(x)}{∂x_2}, . . . ,\frac{∂f(x)}{∂x_n}]^⊤ \\
> (w, b) ← (w, b) −\frac{η}{|B|}
> ∑
> _{i∈B}
> ∂_{(w,b)}
> l
> ^{(i)}
> (w, b)
> $$
> 其中n表示学习率，学习率太小会收敛太慢，学习率太大会出现震荡难以收敛。

##### 1.1 自动求导

在实践过程中，往往需要进行自动的梯度计算，基于求导的**链式法则**，通常有正向计算和反向传递两种方式：

* **链式法则**：

$$

$$

* **自动求导**：计算函数在指定值上的导数

  > [!NOTE]
  >
  > 自动求导有别于**符号求导**（直接显示的计算函数的导数表达式）和**数值求导**（根据函数的数值和导数极限定义计算导数的值）。

  <img src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-07" alt="image-20220107235918270" style="zoom:50%;" />

  在进行自动求导之前，需要将函数分解为单一的操作子，并将计算表示成一个无环图（计算图），基于计算图就可以根据链式法则进行正向累计和反向累计的计算。

  <img src="https://github.com/MLNLP-World/DeepLearning-MuLi-Notes/raw/main/imgs/07/image-12" alt="image-20220108001847175" style="zoom:50%;" />

  * **反向累计**：先前向执行图并存储中间结果（非常占用GPU内存），再根据中间结果反向更新梯度。

    > [!CAUTION]
    >
    > 既然反向累计耗内存，为什么不用正向呢？
    >
    > **正向计算的时间复杂度较高，对于每一个参数的梯度计算都需要将所有的变量扫一遍**

* **pytorch代码实践**

  ```python
  #存储梯度
  x.torch.arrange(4.0)
  x.requires_grad_(True)#等价于x = torch.arange(4.0,requires_grad=True)
  #默认值是None
  #通过调用反向传播函数来自动计算
  y=torch.dot(x,x)
  y.backward()
  # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值,否则就会累加上之前的梯度
  x.grad.zero_()
  ```

##### 1.2 深度学习之反向传播算法

> [!NOTE]
>
> 需要注意的是反向传播算法与自动求导中的反向累计不同，后者是针对每个函数的求导过程，前者则是为了除了深层次的神经网络，使得可以反向的根据后一层的导数计算前一层的导数。

误差项：

![image-20240402162021240](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20240402162021240.png)

![image-20240402162100450](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20240402162100450.png)

反向传播算法的含义是： 第 𝑙 层的一个神经元的误差项（或敏感性）是所有与该神经元相连的第 𝑙 + 1 层 的神经元的误差项的权重和．然后，再乘上该神经元激活函数的梯度．

##### 1.3 梯度优化方法



#### 二、多层感知机及其实现

##### 2.1 开始之从线性回归到softmax回归

**线性回归（Linear Regression）**是一种对自变量和因变量之间关系进行建模的回归分析。从机器学习的角度来看，自变量就是样本的特征向量$𝒙 ∈ ℝ^𝐷$（每一维对应 一个自变量），因变量是标签y。
$$
𝑓(\vec𝒙; \vec{𝒘}, 𝑏) = \vec𝒘^T\vec𝒙 +b
$$
给定一组包含 𝑁 个训练样本的训练集$ 𝒟 = {(𝒙^{(𝑛)}, 𝑦^{(𝑛)})}^𝑁_{𝑛=1}$，我们希望能够学习一个最优的线性回归的模型参数𝒘。对于回归类问题往往使用**平方损失函数（MSE）**。

**从回归问题到分类问题**：在分类问题中，由于输出目标 𝑦 是一些离散的标签，而 𝑓(𝒙; 𝒘) 的值域为实数，因此无法直接用 𝑓(𝒙; 𝒘) 来进行预测，需要引入一个非线性的决策函数 （Decision Function）𝑔(⋅)来预测输出目标。

* 二分类问题

* 多分类问题： “一对其余”，“一对一”，“argmax”

  <img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20240402201500539.png" alt="image-20240402201500539" style="zoom:80%;" />

  > [!IMPORTANT]
  >
  > argmax方法的优点是什么？

  

**Softmax回归**：也称为多项（Multinomial）或多类 （Multi-Class）的Logistic回归，是Logistic回归在多分类问题上的推广，对于多类问题，类别标签𝑦 ∈ {1, 2, ⋯ , 𝐶}可以有𝐶 个取值．给定一个样本𝒙， Softmax回归预测的属于类别𝑐的条件概率为：
$$
𝑝(𝑦 = 𝑐|𝒙) = softmax(𝒘^T_𝑐𝒙) =\frac{exp(𝒘^T_𝑐𝒙)}{∑^𝐶_{i=1 }exp(𝒘^T_ix)}
$$
其中$𝒘_𝑐$​ 是第𝑐类的权重向量．

> [!CAUTION]
>
> 需要注意的是，往往实践中softmax算子指的仅仅是归一化的部分，不包括$𝒘^T_𝑐𝒙$，所以上式其实是一个线性层+softmax层。



##### 2.2 感知机原理

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20240402205111715.png" alt="image-20240402205111715" style="zoom: 33%;" />

从现在的观点来看，感知机实际上就是神经网络中的一个神经单元（含有激活函数）。

> 通常输入层并不算一层神经网络，所以上面图中展示的往往称为一层神经元。

感知机能解决二分类问题，但与线性回归和softmax回归有所区别：线性回归与softmax回归的输出均为实数，softmax回归的输出同时还满足概率公理。

无论是线性回归还是softmax回归，都是一种带有偏置项的线性变换。线性模型的特点是只能以线性的方式对特征空间进行划分

> [!WARNING]
>
> **线性模型可能会出错**
>
> 1. 线性意味着单调假设：任何特征的增⼤都会导致模型输出的增⼤（如果对应的权重为正），或者导致模 型输出的减⼩（如果对应的权重为负）。
> 2. 线性假设意味着特征与预测存在线性相关性，这也是不可靠的.
> 3. 线性模型的评估标准是有位置依赖性的，这是不可靠的。

考虑到这些问题，感知机无法解决XOR问题

**XOR问题的多层次解决**

以XOR问题为例，XOR问题的一个解决思路是分类两次，先按x轴分类为+和-，再按y轴分类为+和-，最后将两个分类结果相乘，+即为一三象限，-即为二四象限。

将多个感知机堆叠起来，形成具有多个层次的结构

但是仅仅有线性变换是不够的，如果我们简单的将多个线性变换按层次叠加，由于线性变换的结果仍为线性变换，所以最终的结果等价于线性变换，与单个感知机并无区别，为了防止这个问题，需要对每个单元（感知机）的输出通过**激活函数**进行处理，这些激活函数就是解决非线性问题的关键。

<img src="C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20240402205701538.png" alt="image-20240402205701538" style="zoom:50%;" />

##### 2.3 机器学习实践开始——实现多层感知机

