---
tags: []
parent: 'Tree of thoughts: Deliberate problem solving with large language models'
collections:
    - 'Reasoning enhancement'
$version: 3715
$libraryID: 1
$itemKey: DU9ABMVR

---
## Tree of thoughts: Deliberate problem solving with large language models

#### â„¹ï¸åŸºæœ¬ä¿¡æ¯

|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **æœŸåˆŠ:**\*\*\*\*\*\*ï¼ˆå‘è¡¨å¹´ä»½:**2023**ï¼‰**ä½œè€…:Shunyu Yao; Dian Yu; Jeffrey Zhao; Izhak Shafran; Thomas L. Griffiths; Yuan Cao; Karthik Narasimhanæœºæ„:**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| **æ‘˜è¦**: Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are\*\*still confined to token-level, left-to-right decision-making processes during inference.\*\*This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference,\*\*Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.\*\*ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts:<https://github.com/princeton-nlp/tree-of-thought-llm>. |
| \*\*Local Link: \*\*[Yao ç­‰ - 2023 - Tree of thoughts Deliberate problem solving with large language models.pdf](zotero://open-pdf/0_8IZWNMDL)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |

#### ğŸ’¡ä¸€ã€ç ”ç©¶å†…å®¹

Language models **still confined to token-level, left-to-right decision-making processes during inference.**

**Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.**

ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.

#### ğŸ“œäºŒã€ç ”ç©¶èƒŒæ™¯

![\<img alt="image-20241012174037032" data-attachment-key="CIDUF5X2" width="915" height="474" src="attachments/CIDUF5X2.png" ztype="zimage">](attachments/CIDUF5X2.png)

å¯¹â€œåŒé‡è¿‡ç¨‹â€æ¨¡å‹çš„ç ”ç©¶è¡¨æ˜ï¼Œäººä»¬æœ‰ä¸¤ç§å‚ä¸å†³ç­–çš„æ¨¡å¼ï¼š

1.  å¿«é€Ÿã€è‡ªåŠ¨ã€æ— æ„è¯†æ¨¡å¼ï¼ˆâ€œç³»ç»Ÿ1â€ï¼‰
2.  ç¼“æ…¢ã€æ·±æ€ç†Ÿè™‘ã€æœ‰æ„è¯†çš„æ¨¡å¼ï¼ˆâ€œç³»ç»Ÿ2â€ï¼‰

ç›®å‰ï¼ŒLLMsåŸºäºtoken-levelçš„æ¨ç†è¿‡ç¨‹ï¼Œå¾€å¾€è¿˜åœç•™åœ¨ç³»ç»Ÿ1ï¼Œè€Œæ›´åŠ ç¼“æ…¢ã€æ·±æ€ç†Ÿè™‘çš„æ¨ç†è¿‡ç¨‹å¯èƒ½å¯¹æ¨¡å‹æœ‰ç›Šçš„ï¼Œå³ï¼šï¼ˆ1ï¼‰ç›¸æ¯”äºä»…ä»…åšå‡ºä¸€ä¸ªå†³ç­–ï¼Œç»´æŠ¤å¹¶æ¢ç´¢å½“å‰é€‰æ‹©çš„å¤šç§ä»£æ›¿æ–¹æ¡ˆï¼›ï¼ˆ2ï¼‰è¯„ä¼°å½“å‰çŠ¶å†µå¹¶ç§¯æå±•æœ›æœªæ¥æˆ–å›æº¯ä»¥åšå‡ºæ›´å…¨é¢çš„å†³ç­–ã€‚

**ç°æœ‰å¤§æ¨¡å‹çš„ç¼ºç‚¹ï¼š1ï¼‰å±€éƒ¨æ€§ï¼šç°æœ‰LMæ¨¡å‹ä¸ä¼šåœ¨æ€ç»´è¿‡ç¨‹ä¸­æ¢ç´¢ä¸åŒçš„å»¶ç»­ï¼Œå³ä¸ä¼šå»æ¢ç´¢å½“å‰èŠ‚ç‚¹çš„å…¶ä»–åˆ†æ”¯è§£å†³æ–¹æ¡ˆã€‚2ï¼‰ç¼ºä¹å…¨å±€æ€§ï¼šå¤§æ¨¡å‹LMæ²¡æœ‰çº³å…¥ä»»ä½•ç±»å‹çš„è§„åˆ’ã€å±•æœ›æˆ–å›æº¯æ¥å¸®åŠ©è¯„ä¼°ä¸åŒçš„é€‰é¡¹ã€‚**

äººç±»æ¨ç†ç‰¹ç‚¹ï¼š**å¯ä»¥é‡å¤ä½¿ç”¨å¯ç”¨çš„ä¿¡æ¯æ¥è¿›è¡Œå¯å‘å¼çš„æ¢ç´¢ï¼Œä¿ƒä½¿æŒ–æ˜å‡ºæ›´å¤šçœŸæ­£æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ‰¾åˆ°æœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆã€‚**

ä¸ºäº†è®¾è®¡å¦‚äººç±»è¿™æ ·çš„å†³ç­–è¿‡ç¨‹ï¼Œä¸€ä¸ªç›´è§‰çš„æ–¹æ¡ˆå°±æ˜¯å°†æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºtree searchçš„å½¢å¼

> Newell and colleagues characterized problem solving as search through a combinatorial problem space, represented as a tree.

å°†è¿™ç§åŸºäºè¯­è¨€çš„èƒ½åŠ›ä¸æœç´¢ç®—æ³•ç›¸ç»“åˆï¼Œä»¥ç”Ÿæˆå’Œè¯„ä¼°å„ç§æƒ³æ³•ï¼Œä¾‹å¦‚å¹¿åº¦ä¼˜å…ˆæœç´¢ (BFS) æˆ–æ·±åº¦ä¼˜å…ˆæœç´¢ (DFS)ï¼Œè¿™äº›ç®—æ³•å…è®¸é€šè¿‡å‰ç»å’Œå›æº¯æ¥ç³»ç»Ÿåœ°æ¢ç´¢æ€ç»´æ ‘ã€‚

#### ğŸ”¬ä¸‰ã€ä¸»è¦æ–¹æ³•

> A genuine problem-solving process involves the repeated use of available information to initiate exploration, which discloses, in turn, more information until a way to attain the solution is finally discovered.â€”â€” Newell et al.

ToTçš„å…·ä½“å®ä¾‹æ¶‰åŠå›ç­”å››ä¸ªé—®é¢˜ï¼š

1.  å¦‚ä½•å°†ä¸­é—´è¿‡ç¨‹åˆ†è§£ä¸ºæ€ç»´æ­¥éª¤ï¼šæ ¹æ®é—®é¢˜ç±»å‹è¿›è¡Œç‰¹å®šçš„è®¾è®¡ï¼Œæ¯ä¸€ä¸ªæ€ç»´ä¸èƒ½å¤ªå¤§ï¼Œä¹Ÿä¸èƒ½å¤ªå°
2.  å¦‚ä½•ä»æ¯ä¸ªçŠ¶æ€ä¸­äº§ç”Ÿæ½œåœ¨çš„æƒ³æ³•ï¼šç›´æ¥ä¸COTç±»ä¼¼è¿›è¡Œé‡‡æ ·ï¼Œæˆ–è€…ä½¿ç”¨propose promptçš„æ–¹å¼
3.  å¦‚ä½•å¯å‘å¼è¯„ä¼°çŠ¶æ€ï¼šç‹¬ç«‹çš„è¯„ä¼°æ¯ä¸ªçŠ¶æ€ï¼Œæˆ–è€…é‡‡ç”¨æŠ•ç¥¨æ–¹å¼è¯„ä¼°é€‰æ‹©å‡ ä¸ªçŠ¶æ€ä¸­æœ€å¥½çš„é‚£ä¸ªã€‚
4.  ä½¿ç”¨ä»€ä¹ˆæœç´¢ç®—æ³•ã€‚

![\<img alt="image-20241012180801289" data-attachment-key="MLB352HV" width="1084" height="366" src="attachments/MLB352HV.png" ztype="zimage">](attachments/MLB352HV.png)

#### ğŸš©å››ã€å®éªŒç»“æœ

![\<img alt="image-20241012182239718" data-attachment-key="WILGDEQA" width="1086" height="377" src="attachments/WILGDEQA.png" ztype="zimage">](attachments/WILGDEQA.png)

#### ğŸ”¬äº”ã€æ€è€ƒ

limitationï¼š

1.  æœ‰äº›åœºæ™¯ç”¨ä¸ä¸Šè¿™æ ·å¤æ‚çš„æ–¹æ³•ï¼Œä¸”thoughtè®¾è®¡èµ·æ¥å…·æœ‰ä¸€å®šçš„challenge
2.  å¤šæ¬¡è°ƒç”¨apiä»£ä»·è¾ƒå¤§
3.  ä¸ä½¿ç”¨è¿™æ ·çš„æ–¹æ³•å¯¹LLMsçš„æ€è€ƒè¿‡ç¨‹è¿›è¡Œå¾®è°ƒï¼Œä¼šå½±å“æ¨¡å‹æ•ˆæœ
